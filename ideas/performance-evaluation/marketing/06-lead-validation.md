# Lead Validation Tactics

## Validation Goals

**Hypothesis to test:**
Engineering and product managers at 15-50 person companies will pay $6-8 PEPM for a performance review tool with self-review gap analysis and peer feedback—if they can set it up in 15 minutes without training.

**Success criteria (GO signal):**
- **Message-market fit:** >20% LinkedIn reply rate with positive sentiment
- **Problem validation:** 6+ out of 10 discovery calls say "I'd pay for this" or give unprompted pricing ask
- **Willingness to pay:** 3+ out of 10 commit to paid beta (even $1/user) or ask "when can I buy?"
- **Channel validation:** 2-3 qualified leads per week by Week 8 (confirmed budget + timeline)

**Fail criteria (PIVOT or KILL):**
- **<10% reply rate** after 50+ LinkedIn outreach messages (wrong ICP or messaging)
- **0 buying intent** after 20 discovery calls ("interesting but not a priority")
- **Price objections dominate:** >70% say "too expensive, we'll stick with Docs" (category doesn't exist)
- **No trigger events:** Can't identify when prospects actively search for solution (market isn't urgent)

---

## Unified Customer Conversation Framework

*Core questions used across Marketing validation, Product interviews, and Sales discovery*

### Foundation Questions (All Conversations)

1. **Current State:** "Walk me through the last time you ran performance reviews for your team."
   - *Listen for:* Tools used, time invested, frustrations, workarounds
   - *Follow-up:* "How long did it take you per person?"

2. **Pain Quantification:** "How much does the current process cost you in time? What about money?"
   - *Listen for:* Hours per review cycle, cost of current tools, opportunity cost
   - *Follow-up:* "If you saved 5 hours per cycle, what would you do with that time?"

3. **Failed Solutions:** "What have you tried to solve this? Why didn't it stick?"
   - *Listen for:* Competitors they evaluated, reasons they rejected them, lingering objections
   - *Follow-up:* "What would need to be true for a tool to work for you?"

4. **Trigger Events:** "When did this become a problem? What triggered you to look for a solution?"
   - *Listen for:* Company growth, bad review experience, employee complaint, new manager role
   - *Follow-up:* "What would make this urgent enough to fix in the next 30 days?"

5. **Decision Process:** "If you found a solution today, what's the process to get it approved?"
   - *Listen for:* Budget authority, approval chain, timeline, procurement process
   - *Follow-up:* "Do you control the budget for team tools, or does someone else approve?"

### Marketing Lens (Validation Focus)

**Emphasis:** Problem validation and willingness to pay

**Additional probes:**
- "How do you currently categorize this problem—is it a 'must-fix' or a 'nice-to-have'?"
- "What other tools were you considering?" (competitive alternatives, market category understanding)
- "If I told you this costs $6-8 per employee per month, what's your gut reaction?"

**Success signal:**
- Unprompted request for pricing, availability, or demo
- Offers to introduce you to other managers with same problem (referrals)
- Asks "when can I start using this?" before you pitch

### Product Lens (Workflow Understanding)

*See `product/05-interview-template.md` for detailed workflow questions*

**Emphasis:** User journey, feature priorities, integration needs

**Additional probes:**
- "Show me the last review document you created—walk me through each section."
- "If you could automate one part of the review process, what would it be?"
- "What tools does your team already use daily?" (integration priorities)

### Sales Lens (Qualification & Closing)

*See `sales/02-discovery-call.md` for qualification scorecard*

**Emphasis:** Budget, authority, timeline, success criteria

**Additional probes:**
- "What's your budget for team management tools?" (BANT qualification)
- "When's your next review cycle?" (timeline urgency)
- "If we got this set up in 15 minutes today, what would success look like in 90 days?"

---

## Validation Experiments

### Experiment 1: Message-Market Fit (LinkedIn Outreach)

**Method:** Send 50 personalized LinkedIn connection requests + follow-ups to ICP

**Sample size:** 50 connection requests → expect 20-30 accepts → 20 follow-up messages

**Time to run:** 2 weeks
- Week 1: Send 25 requests, follow up with accepts
- Week 2: Send 25 requests, follow up with Week 1 + new accepts

**Message variations to test:**
- **Variation A:** Problem-focused ("Saw you manage a team—curious how you handle performance reviews?")
- **Variation B:** Value-share ("Just finished Q4 reviews—biggest breakthrough was showing employees their self-ratings next to mine. Curious how you do reviews?")
- **Variation C:** Direct offer ("Built a tool to help managers run structured reviews without Lattice pricing—would love your feedback")

**Pass criteria:**
- **>20% reply rate** (10+ replies out of 50 messages)
- **>50% positive sentiment** ("Yeah, reviews are a pain" or "I'd love to hear more")
- **3-5 discovery calls booked**

**Fail criteria:**
- **<10% reply rate** (wrong targeting or generic messaging)
- **>50% "not interested" responses** (problem doesn't resonate)
- **Zero calls booked** (no urgency or buying intent)

**What to track:**
- Connection accept rate (target: >30%)
- Reply rate by message variation (which opener works best?)
- Objections mentioned (price, timing, already have solution)
- Calls booked vs total replies (conversion funnel)

---

### Experiment 2: Value Proposition Resonance (Discovery Calls)

**Method:** 10 x 15-minute customer discovery calls with qualified managers

**Sample size:** 10 calls (from Experiment 1 or warm network)

**Time to run:** 2 weeks (5 calls/week)

**Call structure:**
1. **Intro (2 min):** "Thanks for taking the time. I'm building something to help managers run performance reviews—but first, I want to understand how you currently do it."
2. **Discovery (8 min):** Use Foundation Questions above (Current State → Pain → Failed Solutions → Triggers → Decision Process)
3. **Soft pitch (3 min):** "Based on what you shared, here's what we're building: [describe core features]. Gut reaction?"
4. **Close (2 min):** "If we had this ready in 60 days, would you be interested in testing it?" (gauge buying intent)

**Pass criteria:**
- **6+ out of 10 say "I'd pay for this"** (or ask about pricing unprompted)
- **3+ give referrals** ("You should talk to [name] at [company]—they have this problem worse")
- **2+ commit to paid beta** (even $1/user = signal they value it)
- **Clear trigger events identified** (when they search for solution)

**Fail criteria:**
- **Lukewarm responses:** "Interesting, but not a priority right now" (no urgency)
- **Price resistance:** >50% say "too expensive for what it does" (positioning problem or category doesn't exist)
- **No budget:** "I'd have to ask my boss" or "We don't have budget" (wrong ICP—no buying authority)
- **Feature confusion:** "Wait, what does this do exactly?" (positioning is unclear)

**What to track (in Notion/Airtable):**
- Name, Company, Title, Team Size
- Current process (Docs, competitor tool, nothing)
- Pain level (1-10 scale: how painful is current process?)
- Willingness to pay (1-10 scale: would they pay $7 PEPM?)
- Buying authority (Yes/No)
- Timeline (next review cycle date)
- Referrals given (names of other managers)
- Follow-up action (send demo, add to waitlist, keep warm)

---

### Experiment 3: Channel Validation (Community Engagement)

**Method:** Actively engage in 2-3 Slack/Discord communities for 3 weeks

**Sample size:** 50+ messages (answers, DMs, posts) across communities

**Time to run:** 3 weeks
- Week 1: Lurk, understand norms, answer 10 questions
- Week 2: Answer 15 questions, DM 5 people with value
- Week 3: Post 1 insight, DM 10 people, offer demos

**Communities to test:**
- Rands Leadership Slack (#performance, #first-time-manager)
- Manager's Club (if accepted)
- /r/managers and /r/ExperiencedDevs on Reddit

**Tactics:**
1. **Answer questions genuinely:** When someone asks "How do you do peer feedback?", give real advice (not a pitch)
2. **DM value-first:** "Saw your question about reviews—here's a template I use [Google Doc link]. LMK if helpful!"
3. **Soft mention product:** After helping 5-10 times, mention "I built a tool for this—happy to show you if you're curious"

**Pass criteria:**
- **10+ DM conversations** initiated by others ("Can you tell me more about X?")
- **3+ demo requests** from community members
- **Positive sentiment:** Upvotes, reactions, "this is helpful" replies

**Fail criteria:**
- **<5 engagements** after 50+ messages (content isn't resonating)
- **Moderators warn you** about self-promotion (too sales-y)
- **Zero inbound interest** (community isn't a fit)

**What to track:**
- Community name + channel
- Question answered or value shared
- Engagement (upvotes, replies, DMs)
- Demos booked from community
- Referrals given

---

## Signal Tracking

| Signal | Strong Positive (GO) | Weak Positive (Iterate) | Negative (PIVOT) |
|--------|----------------------|-------------------------|------------------|
| **Reply rate (LinkedIn)** | >25% | 10-25% | <10% |
| **Call show rate** | >80% | 50-80% | <50% |
| **"When can I buy?" asked** | Unprompted in call | Asked after demo | Doesn't ask |
| **Referrals offered** | 2+ names given | 1 name given | None offered |
| **Budget mentioned** | Specific number or "we pay for Linear/Notion" | "We have budget for tools" | "Let me check with my boss" |
| **Timeline urgency** | "Next review cycle is in 30-60 days" | "Maybe next quarter" | "Not sure when we'll do reviews" |
| **Feature understanding** | "The gap analysis is exactly what I need" | "Interesting features" | "Wait, what does this do?" |

---

## Red Flags (Kill Signals)

Early warning signs that hypothesis is wrong:

- [ ] **No one responds after 50+ outreach attempts** → Wrong ICP or problem isn't painful
- [ ] **People say "interesting" but won't commit 15 min** → Low urgency, not a real pain
- [ ] **No budget even from "qualified" leads** → Wrong audience (target individual contributors, not managers)
- [ ] **Can't explain value in <30 seconds** → Positioning is unclear, messaging needs work
- [ ] **ICP says "we already solved this with Docs/Sheets"** → Category doesn't exist, tool is overkill
- [ ] **Competitors dominate:** "We use Lattice and love it" → Market is already served, differentiation too weak
- [ ] **No clear trigger events** → Can't identify when prospects actively search (long sales cycle ahead)

**If 3+ red flags appear by Week 8:** Pivot ICP, pivot features, or kill idea.

---

## Validation Milestones (30-Day Plan)

### Week 1-2: Message-Market Fit
- [ ] Send 50 LinkedIn connection requests (Experiment 1)
- [ ] Have 5-8 discovery calls (Experiment 2)
- [ ] Join 2-3 communities, answer 15 questions (Experiment 3)

**Decision point:** If <15% reply rate or 0 calls booked, refine messaging and try 25 more connections before pivoting.

### Week 3-4: Problem Validation
- [ ] Send 50 more LinkedIn requests (test winning message variation)
- [ ] Have 10-15 discovery calls
- [ ] Collect 5+ testimonials or quotes about pain ("Reviews are a nightmare")

**Decision point:** If <6/15 calls show buying intent, re-examine ICP (maybe target agencies, not startups) or pivot features.

### Week 5-8: Willingness to Pay
- [ ] Pitch paid beta to top 10 prospects ("$1/user for 3 months, then $6-8")
- [ ] Track commitments (verbal or email confirmation)
- [ ] Refine pricing based on objections

**GO decision:** If 3+ commit to paid beta + 10+ qualified leads in pipeline → Proceed to build
**PIVOT decision:** If <2 commitments + price objections dominate → Test $4-5 PEPM or different ICP
**KILL decision:** If 0 commitments + "we'll stick with Docs" → Category doesn't exist, kill idea

---

*Last updated: January 27, 2026*
